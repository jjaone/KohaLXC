---
# File: roles/hostnode/tasks/dataenv-setup.yaml
# #############################################################################
# Code is part of the KohaLXC/kohatools Ansible/Bash tooling environment
# for Koha/ILS-development, deployment & database conversion/migration tasks.
# Author: Jukka Aaltonen, Koha-Lappi, Rovaniemi City Library, Lapland/Finland.
# License: GNU General Public License version 3.
# 
# Description: data and conversion environment setup
# - setup hostnodes data/MMT-environment in playbook inventories
# - availability of all required distro packages
# - users and groups for data(set) handling and MMT
# - get MMT/conversion data(sets) from dump-servers [control host]
# - set directories/permissions for dataset extraction/conversions [control host]
# - handle (extract) source files (*,kir) from database dumps [control host]
# - generate TranslationTables (liqlocde_translation) for PP/MMT [control host]
# - sync [control host] KohaLXC-tools w/ hostnode devops
# - push data environment setups/datasets to hostnodes for MMt/conversions
# - prepare remote MMT/conversion based on [control host] dataenv+mmtenv
# ##############################################################################

## Ensure env.* pckgs are installed
# [KohaLXC]:data: Packages for data/MMT environment(s)
- name: data - Ensure all needed packages are installed
  become: yes
  apt: >
    name="{{ item }}"
    update_cache="yes"
    state="present"
    autoremove="yes"
  with_flattened:
    - "{{ hostnode_dataenv_enabled | ternary(hostnode_dataenv_pckgs, '') }}"
  when:
    - item is defined and item != ''

## Ensure 'koha'-user (rssh) exists (by default: koha/1001)
- name: data - Ensure user 'koha' (rssh) exists for Koha-data & transfers
  become: yes
  user: >
    name="{{ hostnode_kohagrp_name }}"
    uid="{{ hostnode_kohagrp_id }}"
    group="{{ hostnode_kohagrp_name }}"
    state="present"
    shell="/usr/bin/rssh"
    generate_ssh_key="no"
    comment="Koha/LXC visitor user"
  when:
    - hostnode_kohagrp_name is defined
    - hostnode_kohagrp_id is defined

## Setup remote data environment rssh.conf (dataenv/rssh settings)
# [KohaLXC]:data(not-local)
# - settings for 'KohaLXC' are in roles/common/defauls/main.yaml
# - dataenv == kohalxc_tooldir/conf.d/env.data/files
- name: data(not-local) - Setup system-wide rssh config for remote KohaLXC/dataenv
  become: yes
  template: >
    src="files/dataenv/rssh.conf.j2"
    dest="/etc/rssh.conf"
    backup="no"
  when:
    - hostnode_environment != "localdev"

## Ensure 'koha'-group exists (by default: koha/1001)
# [KohaLXC]:data
- name: data - Ensure group 'koha' exists for Koha-data & transfers
  become: yes
  group: >
    name="{{ hostnode_kohagrp_name }}"
    gid="{{ hostnode_kohagrp_id }}"
    state="present"
  when:
    - hostnode_kohagrp_name is defined
    - hostnode_kohagrp_id is defined

#- debug: var=kohalxc_datadir,kohalxc_dataset,kohalxc_datasets,hostnode_dataenv_enabled
## Pull/extract any uploads of dataset dumps from datasrv/'koha'
# [KohaLXC]:data(control)
# -from: srv:/home/koha/Works/KohaLappi/{{item}}
# -to: local:{{ kohalxc_works }}/{{ kohalxc_organization }}/kohadata/"
# -item(hostnode_dataenv_datasets): e.g. "Axiell-20161019-Lappi_PallasPro"
- name: data(control) - Pull (rsync) listed dataset dumps from server
  synchronize: >
    mode="pull"
    src="/home/{{ hostnode_kohagrp_name | d('koha') }}/Works/{{ hostnode_kohalxc_organization }}/dump4mmt-{{ item }}"
    dest="{{ kohalxc_datadir }}"
    group="yes"
    perms="yes"
    recursive="yes"
    delete="no"
  with_items: "{{ hostnode_dataenv_datasets | map(attribute='name') | list }}"
  when:
    - kohalxc_datadir is defined
    - hostnode_dataenv_datasets is defined
    - hostnode_dataenv_datasets | map(attribute='name') | first | d() != ""

- debug: var=hostnode_dataenv_datasets
## Ensure dataset extraction/conv sources have subdirs (in control host)
# [KohaLXC]:data(control)
- name: data(control) - Dirs/perms for dataset extraction/conversion sources (PallasPro+Origo)
  file: >
    path="{{ kohalxc_datadir }}/dump4mmt-{{ item.0 }}/source/{{ item.1 }}"
    state="directory"
    recurse="no"
    group="{{ hostnode_kohagrp_id }}"
    mode="u=rwx,g=rwxs,o-rwx"
  with_nested:
    - "{{ hostnode_dataenv_datasets | map(attribute='name') | list }}"
    - [ 'TranslationTables', 'preprocessed', 'scripts', 'target', 'logs', 'valid', 'complete' ]
  when:
    - kohalxc_datadir is defined
    - hostnode_dataenv_datasets is defined
    - hostnode_ppmmtenv_enabled or hostnode_origommtenv_enabled
    - item is defined
  delegate_to: localhost

#- debug: msg="hdd_name_list_pp={{ hostnode_dataenv_datasets | map(attribute='name') | list |  join(' ') | match('.*PallasPro.*') }}"
#- debug: var=hostnode_mmtdata,hostnode_mmtdata|bool
#- debug: var=hostnode_ppmmtdata,hostnode_origommtdata
#- debug: var=hostnode_ppmmtenv_enabled,hostnode_origommtenv_enabled

## Extract/uncompress listed dataset(s) from pulled archives for 'PallasPro'
# [KohaLXC]:data(control),mmt("PallasPro")
# - must use local action/gunzip cause no Ansible module for "uncompress" exists
# - Note: "shopt -s/-u nullglob" would work, but only with /bin/bash
- name: data(control)+ppmmt - Extract PP-db table data from dumps
  local_action: shell for f in {{ kohalxc_datadir }}/dump4mmt-{{ item }}/*.gz; do [ -f "$f" ] || continue; d=$(dirname $f); t=$(basename $f .gz); ( cd $d/source && test ! -f $t && gunzip -vc $f > $t || echo -n "$t," ); done
  with_items: "{{ hostnode_dataenv_datasets | map(attribute='name') | list }}"
  when:
    - kohalxc_datadir is defined
    - item is defined and ("PallasPro" in item)

## Extract/uncompress listed dataset(s) from pulled archives for 'Origo'
# [KohaLXC]:data(control),mmt("Origo")
# - must use local action/gunzip cause no Ansible module for "uncompress" exists
# - Note: "shopt -s/-u nullglob" would work, but only with /bin/bash
- name: data(control)+origommt - Extract Origo-db btable data from dumps
  local_action: shell for f in {{ kohalxc_datadir }}/dump4mmt-{{ item }}/*.zip; do [ -f "$f" ] || continue; d=$(dirname $f); t=$(basename $f .zip); ( cd $d/source && unzip $f || exit 0); done
  with_items: "{{ hostnode_dataenv_datasets | map(attribute='name') | list }}"
  when:
    - kohalxc_datadir is defined
    - item is defined and ("Origo" in item)

## Extract source CSV from Google-docs/spreadsheet
# [KohaLXC]:data(control)/ppmt[default]
# - liqlocde_translation.csv  for 'liqlocde_translation.pm' (PP/MMT)
# - staffaccounts.csv for 'makeStaffAccounts' (PP/MMT)
# - 'File id' and 'Sheet id (gid)' required
- name: data(control)+ppmmt[default] - Extract CSVs for TranslationTables/{liqlocde_translation,staffaccounts} from G-spreadsheet
  vars:
    org: "{{ hostnode_kohalxc_organization }}"
    gDocsUrl: "https://docs.google.com/spreadsheets/d"
    gFileId: "{{ kohalxc_dataenv_gfileid }}"
    gExportFormat: "csv"
  uri:
    url: "{{ gDocsUrl }}/{{ gFileId }}/export?exportFormat={{ gExportFormat }}&gid={{ item.1.gid }}"
    dest: "{{ kohalxc_datadir }}/dump4mmt-{{ item.0 }}/source/TranslationTables/{{ item.1.name }}.csv"
    body_format: raw
    return_content: yes
  with_nested:
    - "{{ hostnode_dataenv_datasets | selectattr('default', 'defined') | map(attribute='name') | list }}"
    - [ { name: 'liqlocde_translation', gid: "{{ kohalxc_dataenv_liqlocde_gid }}" },
        { name: 'staffaccounts', gid: "{{ kohalxc_dataenv_staffaccounts_gid }}" } ]
  when:
    - kohalxc_dataenv_gfileid is defined
    - kohalxc_datadir is defined
    - item.0 is defined and ("PallasPro" in item.0)
  delegate_to: localhost

## For 'liqlocde_translation.pm' (PP/MMT) split source CSV from Google-docs/spreadsheet:
# [KohaLXC]:data(controls),ppmmt
- name: data(control)+ppmmt[default] - Fix delimiters and split source CSV by libraryid
  vars:
    srcName: liqlocde_translation
  local_action: shell cd {{ kohalxc_datadir }}/dump4mmt-{{ item }}/source/TranslationTables && rm -f {{ srcName }}-*.csv && cat {{ srcName }}.csv | tr ',' ';' | awk -F, 'NR>1 {print>"{{ srcName }}-" sprintf("%02d", $1) ".csv"}' && rm -f {{ kohalxc_tooldir }}/ppmmtws/PerlMMT/TranslationTables/{{ srcName }}.pm-map
  with_items: "{{ hostnode_dataenv_datasets | selectattr('default', 'defined') | map(attribute='name') | list }}"
  when:
    - kohalxc_datadir is defined
    - kohalxc_tooldir is defined
    - item is defined and ("PallasPro" in item)

## Run in control host: 'kohatools/ppmmtws/ConversionTools/makeliqlocde.sh' script
# [KohaLXC]:data(control),ppmmt
# - to generate the liqlocde_translation map for $org_unitsLappi (TranslationTable)
- name: data(control)+ppmmt[default] - Generate '$org_Units' mapping for TranslationTables/licloqde_translation.pm-map)
  vars:
    srcName: liqlocde_translation
  environment:
    KOHALXC_DATADIR: "{{ kohalxc_datadir }}"
    KOHALXC_DATASET: "{{ item }}"
  shell: "{{ kohalxc_tooldir }}/ppmmtws/ConversionTools/makeliqlocde.sh {{ srcName }}"
  args:
    chdir: "{{ kohalxc_tooldir }}/ppmmtws/ConversionTools"
    creates: "../PerlMMT/TranslationTables/{{ srcName }}.pm-map"
  with_items: "{{ hostnode_dataenv_datasets | selectattr('default', 'defined') | map(attribute='name') | list }}"
  when:
    - kohalxc_datadir is defined
    - kohalxc_tooldir is defined
    - item is defined and ("PallasPro" in item)
  delegate_to: localhost

## Symlink the 'default' dataset to 'dump4mmt-default' (if inventory specifies it)
- name: data(control)+ppmmt[default] - Make 'dump4mmt-default' symlink to current PP/MMT dataset (as set by host/inventory)
  shell: pwd && ls -ld dump4mmt* && rm -f dump4mmt-pp && ln -s -f dump4mmt-{{item}} dump4mmt-pp && rm -f dump4mmt-default && ln -s -f dump4mmt-pp dump4mmt-default
  args:
    executable: /bin/bash
    chdir: "{{ kohalxc_datadir }}"
  with_items: "{{ hostnode_dataenv_datasets | selectattr('default', 'defined') | map(attribute='name') | list | first }}"
  when:
    - kohalxc_datadir is defined
    - item is defined and ('PallasPro' in item)
  register: cmd_dump4default
  delegate_to: localhost
#- debug: var=cmd_dump4default.results

#  notify: Sync kohalxc
## [TODO]: Instead of using any 'synchronize' task below:
# - forcefully notifyhandler to sync KohaLXC-contents to server/devops
# - e.g. KohaLXC-tools need to be in place for 'dataenv/mmtenv'-stuff later
# - can use '--force-handler' to ensure handler is called (atleast in the end)
#- meta: flush_handlers

## Push possible dataset dump fixes to each source scripts forlder
- name: data(control)+ppmmt[default] - Push templetized dump/dataset fixes to source/scripts folder
#  debug: var=item,item.name,item.src
  template: >
    src="files/dataenv/{{item.script}}.j2"
    dest="{{ kohalxc_datadir }}/dump4mmt-{{item.name}}/source/scripts/{{item.script}}"
    mode="u+rwx,g+rx,o-rwx"
  with_items: "{{ hostnode_dataenv_datasets | selectattr('default', 'defined') | list }}"
  when:
    - kohalxc_datadir is defined
    - item is defined and item.src is defined and item.script is defined
    - item.name is defined and ('PallasPro' in item.name)
  delegate_to: localhost

## Run dataset fix scripts for those MMT-envs and sources that need them
- name: data(control)+ppmmt[default] - Run dataset fixes for specified sources
  environment:
    KOHALXC_DATADIR: "{{ kohalxc_datadir }}"
    KOHALXC_DATASET: "{{ item.name | d('default') }}"
  local_action: shell pwd && echo "== " && ls -l {{ kohalxc_datadir }} && echo "== " && {{ kohalxc_datadir }}/dump4mmt-{{item.name}}/source/scripts/{{item.script}} {{item.src}}
  with_items: "{{ hostnode_dataenv_datasets | selectattr('default', 'defined') | list }}"
  register: cmd_fixsources
  when:
    - kohalxc_datadir is defined
    - item is defined and item.src is defined and item.script is defined
    - item.name is defined and ('PallasPro' in item.name)
#- debug: var=cmd_fixsources.results

#- debug: var=kohalxc_setupdir,kohalxc_datadir,hostnode_kohalxc_works,hostnode_kohalxc_organization,hostnode_dataenv_datasets

## Push kohalxc/default setups and kohadata/dump4mtt-<dumpname>/source* datasets:
# [KohaLXC]:data
# - to: /home/'hostnode_operuser'/Works/<org>/{kohalxc,kohadata}
- name: data - Push KohaLXC lxc-setups & dumped datasets to operuser/Works
  synchronize: >
    mode="push"
    src="{{ item }}"
    dest="{{ hostnode_kohalxc_works }}/{{ hostnode_kohalxc_organization }}"
    owner="no"
    group="yes"
    perms="yes"
    links="yes"
    rsync_timeout="15"
    rsync_opts="--include='**/' --include='/*/default/**' --include='/*/dump4mmt-**' --include='/*/dump4mmt-*/**' --include='/*/dump4mmt-*/source*/**' --exclude='*'"
    recursive="yes"
    delete="no"
  with_items:
    - "{{ kohalxc_setupdir }}"
    - "{{ kohalxc_datadir }}"
  when:
    - hostnode_dataenv_datasets is defined

- name: Does PP/MMT have default/current dataset in data directory?
  stat:
    path: "{{ hostnode_dataenv_datadir }}/dump4mmt-pp"
  register: cmd_dump4mmt_pp
#- debug: msg="About dir/link 'dump4mmt-pp'={{cmd_dump4mmt_pp.stat }},exists={{ cmd_dump4mmt_pp.stat.exists }}"

- name: Does Origo/MMT have default/current dataset in data directory?
  stat:
    path: "{{ hostnode_dataenv_datadir }}/dump4mmt-origo"
  register: cmd_dump4mmt_origo
#- debug: msg="About dir/link 'dump4mmt-origo'={{ cmd_dump4mmt_origo.stat }},exist={{ cmd_dump4mmt_origo.stat.exists }}"

#- debug: var="hostnode_ppmmtenv_enabled,hostnode_origommtenv_enabled"
#- debug: var="hostnode_kohalxc_tooldir,hostnode_dataenv_datadir"
## Push/copy KohaLXC/ppmmt tools and templated default config.xml to target
# Parameters passed to {host,lxc}_config.xml
# - name: name of the config file: {host_config,lxc_config}.xml
# - source: source directory in dump4mmt-xyz folder ('source.0' for fixed sources)
# - organization: library that we are MMT-converting
# - threadCount: number of threads MMT should use
# - debugACL: "DEBUGAuthoritiesCountLimit"
# - chunkSize,starting/endingChunk: number of records in MMT-biblios chunk (start/end)
# - loadExternalRepos: whether to load external repositories or not
# - preprocess: do the 'PreProcess' phase in each import chain
# - {biblios_,items_,holds_,fines_,patrons_,checkouts_,history_,serials_}run: Import chains
- name: data+ppmmt - Setup PP/MMT-tools main configuration (host/lxc_config.xml)
  vars:
    host_src: "{{ hostnode_dataenv_datadir }}/dump4mmt-default/source.0"
    lxc_src: "/home/{{ kohalxc_operuser }}/{{ hostnode_kohalxc_organization }}/kohadata/source.0"
    threadCount: 0
    chunkSize: 5000
    org: "{{ hostnode_kohalxc_organization }}"
    ics: "{{ hostnode_ppmmtenv_importchains | d() }}"
    preprocess: "{{ (ics == '' or 'Preprocess' in ics) | ternary(1,0) }}"
    patrons:    "{{ (ics == '' or 'Patrons' in ics)    | ternary(1,0) }}"
    biblios:    "{{ (ics == '' or 'Biblios' in ics)    | ternary(1,0) }}"
    items:      "{{ (ics == '' or 'Items' in ics)      | ternary(1,0) }}"
    checkouts:  "{{ (ics == '' or 'Checkouts' in ics)  | ternary(1,0) }}"
    holds:      "{{ (ics == '' or 'Holds' in ics)      | ternary(1,0) }}"
    fines:      "{{ (ics == '' or 'Fines' in ics)      | ternary(1,0) }}"
    serials:    "{{ (ics == '' or 'Serials' in ics)    | ternary(1,0) }}"
    history: 0
    patrons_ssnindex: 120000
    loadExtRepos: 0
    debugACL: -7000
  template: >
    src="files/dataenv/config.xml.j2"
    dest="{{ hostnode_kohalxc_tooldir }}/ppmmtws/{{ item.name }}.xml"
    group="{{ hostnode_kohagrp_name }}"
    mode="g+w"
    backup="no"
  with_items:
    - { name: lxc_config, source: "{{ lxc_src }}", startChunk: 0, endChunk: -1 }
    - { name: config_all, source: "{{ host_src }}", startChunk: 0, endChunk: -1 }
  when:
    - hostnode_kohalxc_tooldir is defined
    - hostnode_dataenv_datadir is defined
    - cmd_dump4mmt_pp.stat.exists

- fail:

## Push/copy KohaLXC/ppmmt import.pl+TranslationTables to target
- name: data+ppmmt - Push/copy 'ConversionTools' and 'TranslationTables' to data/ppmmt-enabled hosts
  template: >
    src="files/kohalxc_dataenv/{{ item.name }}.j2"
    dest="{{ hostnode_kohalxc_tooldir }}/ppmmtws/{{ item.dir }}/{{ item.name }}"
    group="{{ hostnode_kohagrp_name }}"
    mode="g+w"
    backup="no"
  with_items:
    - { name: makeStaffAccounts.sh, dir: ConversionTools }
    - { name: cutype_to_borrower_categorycode.pm, dir: PerlMMT/TranslationTables }
    - { name: isil_translation.pm, dir: PerlMMT/TranslationTables }
    - { name: liqlocde_translation.pm, dir: PerlMMT/TranslationTables }
    - { name: material_code_to_itype.pm, dir: PerlMMT/TranslationTables }
  when:
    - hostnode_kohalxc_tooldir is defined
    - hostnode_dataenv_datadir is defined
    - cmd_dump4mmt_pp.stat.exists
  
## Push/copy KohaLXC/ppmmt lib and BibliosImportChain scripts/configs to target
- name: data+ppmmt - Push/copy Perl 'import.pl', 'lib' and '{Biblios,Patrons}ImportChain' scripts to data/ppmmt-enabled envs
  template: >
    src="files/kohalxc_dataenv/{{ item.name }}.j2"
    dest="{{ hostnode_kohalxc_tooldir }}/ppmmtws/PerlMMT/{{ item.dir }}/{{ item.name }}"
    group="{{ hostnode_kohagrp_name }}"
    mode="g+w"
    backup="no"
  with_items:
    - { name: import.pl, dir: . }
    - { name: Item.pm, dir: lib }
    - { name: Record.pm, dir: lib/MARC }
    - { name: Patron.pm, dir: lib }
    - { name: MarcRepair.pm, dir: BibliosImportChain }
    - { name: BuildMARC.pm, dir: BibliosImportChain/FinMARC_Builder }
    - { name: SharedDocidRecordsHandler.pm, dir: BibliosImportChain/FinMARC_Builder }
    - { name: fi2ma.rul, dir: BibliosImportChain/Usemarcon/fi2ma }
    - { name: Instructions.pm, dir: PatronsImportChain }
  when:
    - hostnode_kohalxc_tooldir is defined
    - hostnode_dataenv_datadir is defined
    - cmd_dump4mmt_pp.stat.exists

## Abort play here (for testing and development):
#- fail:
